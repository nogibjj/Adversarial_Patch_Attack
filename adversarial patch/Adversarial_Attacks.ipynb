{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd5uck2_ocbq"
      },
      "source": [
        "# Tutorial 10: Adversarial attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cODKBPQGocbs"
      },
      "source": [
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "**Filled notebook:** \n",
        "[![View notebooks on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb)  \n",
        "**Pre-trained models and dataset:** \n",
        "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial10)\n",
        "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1k01P6w31VOW9TT0gTEP9kog315qyiXCd?usp=sharing)  \n",
        "**Recordings:** \n",
        "[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/uidLtkhZFwY)\n",
        "[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/Dmbz0ffc6Wg)\n",
        "[![YouTube - Part 3](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%203&color=red)](https://youtu.be/0dt2Su-SRpI)    \n",
        "**Author:** Phillip Lippe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hxpaG5Iocbt"
      },
      "source": [
        "In this tutorial, we will discuss adversarial attacks on deep image classification models. As we have seen in many of the previous tutorials so far, Deep Neural Networks are a very powerful tool to recognize patterns in data, and, for example, perform image classification on a human-level. However, we have not tested yet how robust these models actually are. Can we \"trick\" the model and find failure modes? Can we design images that the networks naturally classify incorrectly? Due to the high classification accuracy on unseen test data, we would expect that this can be difficult. However, in 2014, a research group at Google and NYU showed that deep CNNs can be easily fooled, just by adding some salient but carefully constructed noise to the images. For instance, take a look at the example below (figure credit - [Goodfellow et al.](https://arxiv.org/pdf/1412.6572.pdf)):\n",
        "\n",
        "<center width=\"100%\" style=\"padding: 20px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/adversarial_example.svg?raw=1\" width=\"550px\"></center>\n",
        "\n",
        "The image on the left is the original image from ImageNet, and a deep CNN classifies the image correctly as \"panda\" with a class likelihood of 57%. Nevertheless, if we add a little noise to every pixel of the image, the prediction of the model changes completely. Instead of a panda, our CNN tells us that the image contains a \"gibbon\" with the confidence of over 99%. For a human, however, these two images look exactly alike, and you cannot distinguish which one has noise added and which doesn't. While this first seems like a fun game to fool trained networks, it can have a serious impact on the usage of neural networks. More and more deep learning models are used in applications, such as for example autonomous driving. Imagine that someone who gains access to the camera input of the car, could make pedestrians \"disappear\" for the image understanding network by simply adding some noise to the input as shown below (the figure is taken from [J.H. Metzen et al.](https://openaccess.thecvf.com/content_ICCV_2017/papers/Metzen_Universal_Adversarial_Perturbations_ICCV_2017_paper.pdf)). The first row shows the original image with the semantic segmentation output on the right (pedestrians red), while the second row shows the image with small noise and the corresponding segmentation prediction. The pedestrian becomes invisible for the network, and the car would think the road is clear ahead.\n",
        "\n",
        "<center width=\"100%\" style=\"padding: 20px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/adversarial_attacks_cityscapes_3.png?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "Some attack types don't even require to add noise, but minor changes on a stop sign can be already sufficient for the network to recognize it as a \"50km/h\" speed sign ([paper](https://arxiv.org/pdf/1707.08945.pdf), [paper](https://arxiv.org/pdf/1802.06430.pdf)). The consequences of such attacks can be devastating. Hence, every deep learning engineer who designs models for an application should be aware of the possibility of adversarial attacks.\n",
        "\n",
        "To understand what makes CNNs vulnerable to such attacks, we will implement our own adversarial attack strategies in this notebook, and try to fool a deep neural network. Let's being with importing our standard libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "btI7vLGvocbt",
        "outputId": "9d950abe-3514-4af0-b28c-195c6ad8984c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_12634/4107161634.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "Global seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda:0\n",
            "/workspaces/Adversarial_Patch_Attack\n"
          ]
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np \n",
        "import scipy.linalg\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)\n",
        "\n",
        "# from tools\n",
        "%cd ..\n",
        "from tools.resnet20 import ResNetCIFAR\n",
        "from tools.train_util import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtrlyLuaocbv"
      },
      "source": [
        "We have again a few download statements. This includes both a dataset, and a few pretrained patches we will use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f_nZ20-eocbv"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import zipfile\n",
        "# Github URL where the dataset is stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "# Files to download\n",
        "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\"), (CHECKPOINT_PATH, \"patches.zip\")]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for dir_name, file_name in pretrained_files:\n",
        "    file_path = os.path.join(dir_name, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
        "        if file_name.endswith(\".zip\"):\n",
        "            print(\"Unzipping file...\")\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vRgvPW8ocbv"
      },
      "source": [
        "## Deep CNNs on ImageNet\n",
        "\n",
        "For our experiments in this notebook, we will use common CNN architectures trained on the ImageNet dataset. Such models are luckily provided by PyTorch's torchvision package, and hence we just need to load the model of our preference. For the results on the website and default on Google Colab, we use a ResNet34. Feel free to experiment with other architectures as well, the code is mainly independent of the specific architecture we choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GH8cCwOWocbv"
      },
      "outputs": [],
      "source": [
        "# Load CNN architecture pretrained on ImageNet\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# No gradients needed for the network\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCH-6Nt2ocbw"
      },
      "source": [
        "To perform adversarial attacks, we also need a dataset to work on. Given that the CNN model has been trained on ImageNet, it is only fair to perform the attacks on data from ImageNet. For this, we provide a small set of pre-processed images from the original ImageNet dataset (note that this dataset is shared under the same [license](http://image-net.org/download-faq) as the original ImageNet dataset). Specifically, we have 5 images for each of the 1000 labels of the dataset. We can load the data below, and create a corresponding data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../data\n"
          ]
        }
      ],
      "source": [
        "print(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qW83V1Vlocbw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Mean and Std from ImageNet\n",
        "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
        "# No resizing and center crop necessary as images are already preprocessed.\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN,\n",
        "                         std=NORM_STD)\n",
        "])\n",
        "\n",
        "# Load dataset and create data loader\n",
        "#imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
        "#assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
        "#                                     f\"Please make sure to have downloaded the ImageNet dataset here, or change the {DATASET_PATH=} variable.\"\n",
        "#dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=True, download=True, transform=plain_transforms)\n",
        "data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=8)\n",
        "\n",
        "# Get label names of CIFAR10 class\n",
        "label_names = dataset.classes\n",
        "print(label_names)\n",
        "\n",
        "# Get label index of CIFAR10 class\n",
        "def get_label_index(lab_str):\n",
        "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
        "    return label_names.index(lab_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# try get_label_index\n",
        "get_label_index('cat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N13cZd-Nocbw"
      },
      "source": [
        "Before we start with our attacks, we should verify the performance of our model. As ImageNet has 1000 classes, simply looking at the accuracy is not sufficient to tell the performance of a model. Imagine a model that always predicts the true label as the second-highest class in its softmax output. Although we would say it recognizes the object in the image, it achieves an accuracy of 0. In ImageNet with 1000 classes, there is not always one clear label we can assign an image to. This is why for image classifications over so many classes, a common alternative metric is \"Top-5 accuracy\", which tells us how many times the true label has been within the 5 most-likely predictions of the model. As models usually perform quite well on those, we report the error (1 - accuracy) instead of the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kySDWdLJocbw"
      },
      "outputs": [],
      "source": [
        "def eval_model(dataset_loader, img_func=None):\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    for imgs, labels in tqdm(dataset_loader, desc=\"Validating...\"):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        if img_func is not None:\n",
        "            imgs = img_func(imgs, labels) \n",
        "        with torch.no_grad():\n",
        "            preds = pretrained_model(imgs)\n",
        "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
        "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        counter += preds.shape[0]\n",
        "    acc = tp.float().item()/counter\n",
        "    top5 = tp_5.float().item()/counter\n",
        "    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n",
        "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "570f009b72a84eee807c65eecd83c735"
          ]
        },
        "id": "vxCZnrRbocbx",
        "outputId": "f99db999-f4fb-4e29-f8d0-1a76b4e7550e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68303ae9121642b2849a8036c96fae61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating...:   0%|          | 0/391 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 error: 100.00%\n",
            "Top-5 error: 99.93%\n"
          ]
        }
      ],
      "source": [
        "_ = eval_model(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p_hdRkrocbx"
      },
      "source": [
        "The ResNet34 achives a decent error rate of 4.3% for the top-5 predictions. Next, we can look at some predictions of the model to get more familiar with the dataset. The function below plots an image along with a bar diagram of its predictions. We also prepare it to show adversarial examples for later applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrained_model = ResNetCIFAR(num_layers=20, Nbits=None)\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "pretrained_model.load_state_dict(torch.load(\"/workspaces/Adversarial_Patch_Attack/tools/pretrained_model_resnet20.pt\",map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# show prediction of a single image\n",
        "def show_prediction(img, label, pred):\n",
        "    plt.imshow(img.permute(1, 2, 0).numpy())\n",
        "    plt.title(f\"Label: {label_names[label]}\\nPrediction: {label_names[pred]}\")\n",
        "    plt.show()\n",
        "# show predictions of a batch of images\n",
        "def show_predictions(imgs, labels, preds):\n",
        "    for i in range(imgs.shape[0]):\n",
        "        show_prediction(imgs[i], labels[i], preds[i])\n",
        "\n",
        "# visualize prediction of model \n",
        "imgs, labels = next(iter(data_loader))\n",
        "imgs = imgs.to(device)\n",
        "labels = labels.to(device)\n",
        "with torch.no_grad():\n",
        "    preds = pretrained_model(imgs)\n",
        "show_predictions(imgs.cpu(), labels.cpu(), preds.argmax(dim=-1).cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RKJf8Oxiocbx"
      },
      "outputs": [],
      "source": [
        "def show_prediction(img, label, pred, K=3, adv_img=None, noise=None):\n",
        "    \n",
        "    if isinstance(img, torch.Tensor):\n",
        "        # Tensor image to numpy\n",
        "        img = img.cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "        label = label.item()\n",
        "    \n",
        "    # Plot on the left the image with the true label as title.\n",
        "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
        "    if noise is None or adv_img is None:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
        "    \n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title(label_names[label])\n",
        "    ax[0].axis('off')\n",
        "    \n",
        "    if adv_img is not None and noise is not None:\n",
        "        # Visualize adversarial images\n",
        "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
        "        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
        "        ax[1].imshow(adv_img)\n",
        "        ax[1].set_title('Adversarial')\n",
        "        ax[1].axis('off')\n",
        "        # Visualize noise\n",
        "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
        "        noise = noise * 0.5 + 0.5 # Scale between 0 to 1 \n",
        "        ax[2].imshow(noise)\n",
        "        ax[2].set_title('Noise')\n",
        "        ax[2].axis('off')\n",
        "        # buffer\n",
        "        ax[3].axis('off')\n",
        "    \n",
        "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
        "        pred = torch.softmax(pred, dim=-1)\n",
        "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
        "    print(topk_idx)\n",
        "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
        "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
        "    ax[-1].set_yticks(np.arange(K))\n",
        "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
        "    ax[-1].invert_yaxis()\n",
        "    ax[-1].set_xlabel('Confidence')\n",
        "    ax[-1].set_title('Predictions')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I91-Ufh8ocbx"
      },
      "source": [
        "Let's visualize a few images below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 30, 509, 582], device='cuda:0')\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     preds \u001b[39m=\u001b[39m pretrained_model(exmp_batch\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m----> 5\u001b[0m show_prediction(exmp_batch[\u001b[39m0\u001b[39;49m], label_batch[\u001b[39m0\u001b[39;49m], preds[\u001b[39m0\u001b[39;49m])\n",
            "Cell \u001b[0;32mIn[20], line 45\u001b[0m, in \u001b[0;36mshow_prediction\u001b[0;34m(img, label, pred, K, adv_img, noise)\u001b[0m\n\u001b[1;32m     43\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbarh(np\u001b[39m.\u001b[39marange(K), topk_vals\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m, align\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mC0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m topk_idx[i]\u001b[39m!=\u001b[39mlabel \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mC2\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K)])\n\u001b[1;32m     44\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticks(np\u001b[39m.\u001b[39marange(K))\n\u001b[0;32m---> 45\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticklabels([label_names[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m topk_idx])\n\u001b[1;32m     46\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39minvert_yaxis()\n\u001b[1;32m     47\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mConfidence\u001b[39m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[20], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbarh(np\u001b[39m.\u001b[39marange(K), topk_vals\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m, align\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mC0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m topk_idx[i]\u001b[39m!=\u001b[39mlabel \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mC2\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K)])\n\u001b[1;32m     44\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticks(np\u001b[39m.\u001b[39marange(K))\n\u001b[0;32m---> 45\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticklabels([label_names[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m topk_idx])\n\u001b[1;32m     46\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39minvert_yaxis()\n\u001b[1;32m     47\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mConfidence\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "data": {
            "application/pdf": "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\n",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"501.021818pt\" height=\"160.544062pt\" viewBox=\"0 0 501.021818 160.544062\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-12-12T05:59:26.378543</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 160.544062 \nL 501.021818 160.544062 \nL 501.021818 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#pe1a2cd9f8c)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"imaged01349a238\" transform=\"scale(1 -1) translate(0 -110.88)\" x=\"7.2\" y=\"-22.318125\" width=\"111.6\" height=\"110.88\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- frog -->\n    <g style=\"fill: #262626\" transform=\"translate(50.715938 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-66\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"35.205078\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"74.068359\"/>\n     <use xlink:href=\"#DejaVuSans-67\" x=\"135.25\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_2\">\n    <path d=\"M 240.185455 133.198125 \nL 493.821818 133.198125 \nL 493.821818 22.318125 \nL 240.185455 22.318125 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 240.185455 133.198125 \nL 240.185455 22.318125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(236.68608 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path d=\"M 300.81966 133.198125 \nL 300.81966 22.318125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5 -->\n      <g style=\"fill: #262626\" transform=\"translate(297.320285 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path d=\"M 361.453866 133.198125 \nL 361.453866 22.318125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 10 -->\n      <g style=\"fill: #262626\" transform=\"translate(354.455116 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path d=\"M 422.088071 133.198125 \nL 422.088071 22.318125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 15 -->\n      <g style=\"fill: #262626\" transform=\"translate(415.089321 151.056406) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path d=\"M 482.722277 133.198125 \nL 482.722277 22.318125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 20 -->\n      <g style=\"fill: #262626\" transform=\"translate(475.723527 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <path d=\"M 240.185455 113.758125 \nL 493.821818 113.758125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 117.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <path d=\"M 240.185455 77.758125 \nL 493.821818 77.758125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 81.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <path d=\"M 240.185455 41.758125 \nL 493.821818 41.758125 \n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 45.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 240.185455 128.158125 \nL 481.743896 128.158125 \nL 481.743896 99.358125 \nL 240.185455 99.358125 \nz\n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 240.185455 92.158125 \nL 463.973909 92.158125 \nL 463.973909 63.358125 \nL 240.185455 63.358125 \nz\n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 240.185455 56.158125 \nL 344.197425 56.158125 \nL 344.197425 27.358125 \nL 240.185455 27.358125 \nz\n\" clip-path=\"url(#pc57aa9e7a0)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 240.185455 133.198125 \nL 240.185455 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 493.821818 133.198125 \nL 493.821818 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 240.185455 133.198125 \nL 493.821818 133.198125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 240.185455 22.318125 \nL 493.821818 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe1a2cd9f8c\">\n   <rect x=\"7.2\" y=\"22.318125\" width=\"110.88\" height=\"110.88\"/>\n  </clipPath>\n  <clipPath id=\"pc57aa9e7a0\">\n   <rect x=\"240.185455\" y=\"22.318125\" width=\"253.636364\" height=\"110.88\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 1000x200 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualize the prediction of the model\n",
        "exmp_batch, label_batch = next(iter(data_loader))\n",
        "with torch.no_grad():\n",
        "    preds = pretrained_model(exmp_batch.to(device))\n",
        "show_prediction(exmp_batch[0], label_batch[0], preds[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b6L9ha08ocbx",
        "outputId": "95f1df98-2f39-4ab6-d217-0b7701d823a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([540, 454, 624, 724, 449], device='cuda:0')\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     preds \u001b[39m=\u001b[39m pretrained_model(exmp_batch\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m17\u001b[39m,\u001b[39m5\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     show_prediction(exmp_batch[i], label_batch[i], preds[i])\n",
            "Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mshow_prediction\u001b[0;34m(img, label, pred, K, adv_img, noise)\u001b[0m\n\u001b[1;32m     43\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbarh(np\u001b[39m.\u001b[39marange(K), topk_vals\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m, align\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mC0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m topk_idx[i]\u001b[39m!=\u001b[39mlabel \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mC2\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K)])\n\u001b[1;32m     44\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticks(np\u001b[39m.\u001b[39marange(K))\n\u001b[0;32m---> 45\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticklabels([label_names[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m topk_idx])\n\u001b[1;32m     46\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39minvert_yaxis()\n\u001b[1;32m     47\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mConfidence\u001b[39m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbarh(np\u001b[39m.\u001b[39marange(K), topk_vals\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m, align\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mC0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m topk_idx[i]\u001b[39m!=\u001b[39mlabel \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mC2\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K)])\n\u001b[1;32m     44\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticks(np\u001b[39m.\u001b[39marange(K))\n\u001b[0;32m---> 45\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_yticklabels([label_names[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m topk_idx])\n\u001b[1;32m     46\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39minvert_yaxis()\n\u001b[1;32m     47\u001b[0m ax[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mConfidence\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "data": {
            "application/pdf": "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\n",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"501.021818pt\" height=\"160.544062pt\" viewBox=\"0 0 501.021818 160.544062\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-12-12T05:56:01.787126</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 160.544062 \nL 501.021818 160.544062 \nL 501.021818 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#p89253b5e77)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image93bb013903\" transform=\"scale(1 -1) translate(0 -110.88)\" x=\"7.2\" y=\"-22.318125\" width=\"111.6\" height=\"110.88\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- truck -->\n    <g style=\"fill: #262626\" transform=\"translate(47.245313 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \nL 1159 4863 \nL 1159 1991 \nL 2875 3500 \nL 3609 3500 \nL 1753 1863 \nL 3688 0 \nL 2938 0 \nL 1159 1709 \nL 1159 0 \nL 581 0 \nL 581 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n     <use xlink:href=\"#DejaVuSans-75\" x=\"80.322266\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"143.701172\"/>\n     <use xlink:href=\"#DejaVuSans-6b\" x=\"198.681641\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_2\">\n    <path d=\"M 240.185455 133.198125 \nL 493.821818 133.198125 \nL 493.821818 22.318125 \nL 240.185455 22.318125 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 240.185455 133.198125 \nL 240.185455 22.318125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(236.68608 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path d=\"M 284.641277 133.198125 \nL 284.641277 22.318125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g style=\"fill: #262626\" transform=\"translate(277.642527 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path d=\"M 329.0971 133.198125 \nL 329.0971 22.318125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 20 -->\n      <g style=\"fill: #262626\" transform=\"translate(322.09835 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path d=\"M 373.552923 133.198125 \nL 373.552923 22.318125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 30 -->\n      <g style=\"fill: #262626\" transform=\"translate(366.554173 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path d=\"M 418.008745 133.198125 \nL 418.008745 22.318125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 40 -->\n      <g style=\"fill: #262626\" transform=\"translate(411.009995 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path d=\"M 462.464568 133.198125 \nL 462.464568 22.318125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 50 -->\n      <g style=\"fill: #262626\" transform=\"translate(455.465818 151.056406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path d=\"M 240.185455 119.758125 \nL 493.821818 119.758125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 123.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <path d=\"M 240.185455 98.758125 \nL 493.821818 98.758125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 102.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <path d=\"M 240.185455 77.758125 \nL 493.821818 77.758125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 81.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <path d=\"M 240.185455 56.758125 \nL 493.821818 56.758125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 60.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <path d=\"M 240.185455 35.758125 \nL 493.821818 35.758125 \n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 4 -->\n      <g style=\"fill: #262626\" transform=\"translate(223.686705 39.937266) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 240.185455 128.158125 \nL 481.743896 128.158125 \nL 481.743896 111.358125 \nL 240.185455 111.358125 \nz\n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 240.185455 107.158125 \nL 306.580774 107.158125 \nL 306.580774 90.358125 \nL 240.185455 90.358125 \nz\n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 240.185455 86.158125 \nL 274.493331 86.158125 \nL 274.493331 69.358125 \nL 240.185455 69.358125 \nz\n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 240.185455 65.158125 \nL 257.90688 65.158125 \nL 257.90688 48.358125 \nL 240.185455 48.358125 \nz\n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 240.185455 44.158125 \nL 250.730942 44.158125 \nL 250.730942 27.358125 \nL 240.185455 27.358125 \nz\n\" clip-path=\"url(#pa7ee3b918b)\" style=\"fill: #4c72b0; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 240.185455 133.198125 \nL 240.185455 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 493.821818 133.198125 \nL 493.821818 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 240.185455 133.198125 \nL 493.821818 133.198125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 240.185455 22.318125 \nL 493.821818 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p89253b5e77\">\n   <rect x=\"7.2\" y=\"22.318125\" width=\"110.88\" height=\"110.88\"/>\n  </clipPath>\n  <clipPath id=\"pa7ee3b918b\">\n   <rect x=\"240.185455\" y=\"22.318125\" width=\"253.636364\" height=\"110.88\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 1000x200 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "exmp_batch, label_batch = next(iter(data_loader))\n",
        "with torch.no_grad():\n",
        "    preds = pretrained_model(exmp_batch.to(device))\n",
        "\n",
        "for i in range(1,17,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCc9qVNsocbx"
      },
      "source": [
        "The bar plot on the right shows the top-5 predictions of the model with their class probabilities. We denote the class probabilities with \"confidence\" as it somewhat resembles how confident the network is that the image is of one specific class. Some of the images have a highly peaked probability distribution, and we would expect the model to be rather robust against noise for those. However, we will see below that this is not always the case. Note that all of the images are of fish because the data loader doesn't shuffle the dataset. Otherwise, we would get different images every time we run the notebook, which would make it hard to discuss the results on the static version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl2zFaymocby"
      },
      "source": [
        "## White-box adversarial attacks\n",
        "\n",
        "There have been proposed many possible adversarial attack strategies, which all share the same goal: alternate the data/image input only a little bit to have a great impact on the model's prediction. Specifically, if we look at the ImageNet predictions above, how can we have to change the image of the goldfish so that the model does not recognize the goldfish anymore? At the same time, the label of the image should not change, in the sense that a human would still clearly classify it as a goldfish. This is the same objective that the generator network has in the Generative Adversarial Network framework: try to fool another network (discriminator) by changing its input.\n",
        "\n",
        "Adversarial attacks are usually grouped into \"white-box\" and \"black-box\" attacks. White-box attacks assume that we have access to the model parameter and can, for example, calculate the gradients with respect to the input (similar as in GANs). Black-box attacks on the other hand have the harder task of not having any knowledge about the network, and can only obtain predictions for an image, but no gradients or the like. In this notebook, we will focus on white-box attacks as they are usually easier to implement and follow the intuition of Generative Adversarial Networks (GAN) as studied in lecture 10.\n",
        "\n",
        "### Fast Gradient Sign Method (FGSM)\n",
        "\n",
        "One of the first attack strategies proposed is Fast Gradient Sign Method (FGSM), developed by [Ian Goodfellow et al.](https://arxiv.org/pdf/1412.6572.pdf) in 2014. Given an image, we create an adversarial example by the following expression:\n",
        "\n",
        "$$\\tilde{x} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta,x,y))$$\n",
        "\n",
        "The term $J(\\theta,x,y)$ represents the loss of the network for classifying input image $x$ as label $y$; $\\epsilon$ is the intensity of the noise, and $\\tilde{x}$ the final adversarial example. The equation resembles SGD and is actually nothing else than that. We change the input image $x$ in the direction of *maximizing* the loss $J(\\theta,x,y)$. This is exactly the other way round as during training, where we try to minimize the loss. The sign function and $\\epsilon$ can be seen as gradient clipping and learning rate specifically. We only allow our attack to change each pixel value by $\\epsilon$. You can also see that the attack can be performed very fast, as it only requires a single forward and backward pass. Let's implement it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWLypKX3ocby"
      },
      "outputs": [],
      "source": [
        "def fast_gradient_sign_method(model, imgs, labels, epsilon=0.02):\n",
        "    # Determine prediction of the model\n",
        "    inp_imgs = imgs.clone().requires_grad_()\n",
        "    preds = model(inp_imgs.to(device))\n",
        "    preds = F.log_softmax(preds, dim=-1)\n",
        "    # Calculate loss by NLL\n",
        "    loss = -torch.gather(preds, 1, labels.to(device).unsqueeze(dim=-1))\n",
        "    loss.sum().backward()\n",
        "    # Update image to adversarial example as written above\n",
        "    noise_grad = torch.sign(inp_imgs.grad.to(imgs.device))\n",
        "    fake_imgs = imgs + epsilon * noise_grad\n",
        "    fake_imgs.detach_()\n",
        "    return fake_imgs, noise_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcgS5SeLocby"
      },
      "source": [
        "The default value of $\\epsilon=0.02$ corresponds to changing a pixel value by about 1 in the range of 0 to 255, e.g. changing 127 to 128. This difference is marginal and can often not be recognized by humans. Let's try it below on our example images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLY9jindocby",
        "outputId": "04a825e8-8315-4deb-f82f-2e4b5aad1a1d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "adv_imgs, noise_grad = fast_gradient_sign_method(pretrained_model, exmp_batch, label_batch, epsilon=0.02)\n",
        "with torch.no_grad():\n",
        "    adv_preds = pretrained_model(adv_imgs.to(device))\n",
        "    \n",
        "for i in range(1,17,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds[i], adv_img=adv_imgs[i], noise=noise_grad[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py8nvelYocby"
      },
      "source": [
        "Despite the minor amount of noise, we are able to fool the network on all of our examples. None of the labels have made it into the top-5 for the four images, showing that we indeed fooled the model. We can also check the accuracy of the model on the adversarial images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "50d6d8a1d98b49a7afd8fd6c25927c29"
          ]
        },
        "id": "E-l-uNLSocby",
        "outputId": "91137ed6-a9f8-4315-8b98-4ca9a5cb21ea"
      },
      "outputs": [],
      "source": [
        "_ = eval_model(data_loader, img_func=lambda x, y: fast_gradient_sign_method(pretrained_model, x, y, epsilon=0.02)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msCKgT29ocby"
      },
      "source": [
        "As expected, the model is fooled on almost every image at least for the top-1 error, and more than half don't have the true label in their top-5. This is a quite significant difference compared to the error rate of 4.3% on the clean images. However, note that the predictions remain semantically similar. For instance, in the images we visualized above, the tench is still recognized as another fish, as well as the great white shark being a dugong. FGSM could be adapted to increase the probability of a specific class instead of minimizing the probability of a label, but for those, there are usually better attacks such as the adversarial patch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfgvMq4Focby"
      },
      "source": [
        "### Adversarial Patches\n",
        "\n",
        "Instead of changing every pixel by a little bit, we can also try to change a small part of the image into whatever values we would like. In other words, we will create a small image patch that covers a minor part of the original image but causes the model to confidentially predict a specific class we choose. This form of attack is an even bigger threat in real-world applications than FSGM. Imagine a network in an autonomous car that receives a live image from a camera. Another driver could print out a specific pattern and put it on the back of his/her vehicle to make the autonomous car believe that the car is actually a pedestrian. Meanwhile, humans would not notice it. [Tom Brown et al.](https://arxiv.org/pdf/1712.09665.pdf) proposed a way of learning such adversarial image patches robustly in 2017 and provided a short demonstration on [YouTube](https://youtu.be/i1sp4X57TL4). Interestingly, if you add a small picture of the target class (here *toaster*) to the original image, the model does not pick it up at all. A specifically designed patch, however, which only roughly looks like a toaster, can change the network's prediction instantaneously.\n",
        "\n",
        "[![Adversarial patch in real world](https://img.youtube.com/vi/i1sp4X57TL4/0.jpg)](https://youtu.be/i1sp4X57TL4)\n",
        "\n",
        "Let's take a closer look at how we can actually train such patches. The general idea is very similar to FSGM in the sense that we calculate gradients for the input, and update our adversarial input correspondingly. However, there are also some differences in the setup. Firstly, we do not calculate a gradient for every pixel. Instead, we replace parts of the input image with our patch and then calculate the gradients just for our patch. Secondly, we don't just do it for one image, but we want the patch to work with any possible image. Hence, we have a whole training loop where we train the patch using SGD. Lastly, image patches are usually designed to make the model predict a specific class, not just any other arbitrary class except the true label. For instance, we can try to create a patch for the class \"toaster\" and train the patch so that our pretrained model predicts the class \"toaster\" for any image with the patch in it.\n",
        "\n",
        "Additionally, to the setup described above, there are a couple of design choices we can take. For instance, [Brown et al.](https://arxiv.org/pdf/1712.09665.pdf) randomly rotated and scaled the patch during training before placing it at a random position in an input image. This makes the patch more robust to small changes and is necessary if we want to fool a neural network in a real-world application. For simplicity, we will only focus on making the patch robust to the location in the image. Given a batch of input images and a patch, we can add the patch as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BU6tbkCmocby"
      },
      "outputs": [],
      "source": [
        "def place_patch(img, patch):\n",
        "    for i in range(img.shape[0]):\n",
        "        h_offset = np.random.randint(0,img.shape[2]-patch.shape[1]-1)\n",
        "        w_offset = np.random.randint(0,img.shape[3]-patch.shape[2]-1)\n",
        "        img[i,:,h_offset:h_offset+patch.shape[1],w_offset:w_offset+patch.shape[2]] = patch_forward(patch)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9EUlOLTocby"
      },
      "source": [
        "The patch itself will be an `nn.Parameter` whose values are in the range between $-\\infty$ and $\\infty$. Images are, however, naturally limited in their range, and thus we write a small function that maps the parameter into the image value range of ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QTuTZQ15ocby"
      },
      "outputs": [],
      "source": [
        "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
        "def patch_forward(patch):\n",
        "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
        "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
        "    return patch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP2XhwPKocbz"
      },
      "source": [
        "Before looking at the actual training code, we can write a small evaluation function. We evaluate the success of a patch by how many times we were able to fool the network into predicting our target class. A simple function for this is implemented below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YpjUc1rfocbz"
      },
      "outputs": [],
      "source": [
        "def eval_patch(model, patch, val_loader, target_class):\n",
        "    model.eval()\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
        "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
        "            for _ in range(4): \n",
        "                patch_img = place_patch(img, patch)\n",
        "                patch_img = patch_img.to(device)\n",
        "                img_labels = img_labels.to(device)\n",
        "                pred = model(patch_img)\n",
        "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
        "                # as we would not \"fool\" the model into predicting those\n",
        "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
        "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
        "                counter += (img_labels != target_class).sum()\n",
        "    acc = tp/counter\n",
        "    top5 = tp_5/counter\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Ui7du6ocbz"
      },
      "source": [
        "Finally, we can look at the training loop. Given a model to fool, a target class to design the patch for, and a size $k$ of the patch in the number of pixels, we first start by creating a parameter of size $3\\times k\\times k$. These are the only parameters we will train, and the network itself remains untouched. We use a simple SGD optimizer with momentum to minimize the classification loss of the model given the patch in the image. While we first start with a very high loss due to the good initial performance of the network, the loss quickly decreases once we start changing the patch. In the end, the patch will represent patterns that are characteristic of the class. For instance, if we would want the model to predict a \"goldfish\" in every image, we would expect the pattern to look somewhat like a goldfish. Over the iterations, the model finetunes the pattern and, hopefully, achieves a high fooling accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NLdojBB2ocbz"
      },
      "outputs": [],
      "source": [
        "def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n",
        "    # Leave a small set of images out to check generalization\n",
        "    # In most of our experiments, the performance on the hold-out data points\n",
        "    # was as good as on the training set. Overfitting was little possible due\n",
        "    # to the small size of the patches.\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
        "    \n",
        "    # Create parameter and optimizer\n",
        "    if not isinstance(patch_size, tuple):\n",
        "        patch_size = (patch_size, patch_size)\n",
        "    patch = nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1]), requires_grad=True)\n",
        "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        t = tqdm(train_loader, leave=False)\n",
        "        for img, _ in t:\n",
        "            img = place_patch(img, patch)\n",
        "            img = img.to(device)\n",
        "            pred = model(img)\n",
        "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
        "            loss = loss_module(pred, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
        "    \n",
        "    # Final validation\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "    \n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y63bNeziocbz"
      },
      "source": [
        "To get some experience with what to expect from an adversarial patch attack, we want to train multiple patches for different classes. As the training of a patch can take one or two minutes on a GPU, we have provided a couple of pre-trained patches including their results on the full dataset. The results are saved in a JSON file, which is loaded below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QXSIzNWsocbz"
      },
      "outputs": [],
      "source": [
        "# Load evaluation results of the pretrained patches\n",
        "json_results_file = os.path.join(CHECKPOINT_PATH, \"patch_results.json\")\n",
        "json_results = {}\n",
        "if os.path.isfile(json_results_file):\n",
        "    with open(json_results_file, \"r\") as f:\n",
        "        json_results = json.load(f)\n",
        "        \n",
        "# If you train new patches, you can save the results via calling this function\n",
        "def save_results(patch_dict):\n",
        "    result_dict = {cname: {psize: [t.item() if isinstance(t, torch.Tensor) else t \n",
        "                                   for t in patch_dict[cname][psize][\"results\"]] \n",
        "                           for psize in patch_dict[cname]} \n",
        "                   for cname in patch_dict}\n",
        "    with open(os.path.join(CHECKPOINT_PATH, \"patch_results.json\"), \"w\") as f:\n",
        "        json.dump(result_dict, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT2Zj9uUocbz"
      },
      "source": [
        "Additionally, we implement a function to train and evaluate patches for a list of classes and patch sizes. The pretrained patches include the classes *toaster*, *goldfish*, *school bus*, *lipstick*, and *pineapple*. We chose the classes arbitrarily to cover multiple domains (animals, vehicles, fruits, devices, etc.). We trained each class for three different patch sizes: $32\\times32$ pixels, $48\\times48$ pixels, and $64\\times64$ pixels. We can load them in the two cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2hRC25Vvocbz"
      },
      "outputs": [],
      "source": [
        "def get_patches(class_names, patch_sizes):\n",
        "    result_dict = dict()\n",
        "\n",
        "    # Loop over all classes and patch sizes\n",
        "    for name in class_names:\n",
        "        result_dict[name] = dict()\n",
        "        for patch_size in patch_sizes:\n",
        "            c = label_names.index(name)\n",
        "            file_name = os.path.join(CHECKPOINT_PATH, f\"{name}_{patch_size}_patch.pt\")\n",
        "            # Load patch if pretrained file exists, otherwise start training\n",
        "            if not os.path.isfile(file_name):\n",
        "                patch, val_results = patch_attack(pretrained_model, target_class=c, patch_size=patch_size, num_epochs=5)\n",
        "                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n",
        "                torch.save(patch, file_name)\n",
        "            else:\n",
        "                patch = torch.load(file_name)\n",
        "            # Load evaluation results if exist, otherwise manually evaluate the patch\n",
        "            if name in json_results:\n",
        "                results = json_results[name][str(patch_size)]\n",
        "            else:\n",
        "                results = eval_patch(pretrained_model, patch, data_loader, target_class=c)    \n",
        "            \n",
        "            # Store results and the patches in a dict for better access\n",
        "            result_dict[name][patch_size] = {\n",
        "                \"results\": results,\n",
        "                \"patch\": patch\n",
        "            }\n",
        "        \n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXQRwA4Wocbz"
      },
      "source": [
        "Feel free to add any additional classes and/or patch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fjh8NVD6ocbz"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Sum of input lengths does not equal the length of the input dataset!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m class_names \u001b[39m=\u001b[39m label_names\n\u001b[1;32m      3\u001b[0m patch_sizes \u001b[39m=\u001b[39m [\u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m16\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m patch_dict \u001b[39m=\u001b[39m get_patches(class_names, patch_sizes)\n\u001b[1;32m      6\u001b[0m save_results(patch_dict) \u001b[39m# Uncomment if you add new class names and want to save the new results\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mget_patches\u001b[0;34m(class_names, patch_sizes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Load patch if pretrained file exists, otherwise start training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(file_name):\n\u001b[0;32m---> 12\u001b[0m     patch, val_results \u001b[39m=\u001b[39m patch_attack(pretrained_model, target_class\u001b[39m=\u001b[39;49mc, patch_size\u001b[39m=\u001b[39;49mpatch_size, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation results for \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mpatch_size\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m, val_results)\n\u001b[1;32m     14\u001b[0m     torch\u001b[39m.\u001b[39msave(patch, file_name)\n",
            "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mpatch_attack\u001b[0;34m(model, target_class, patch_size, num_epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpatch_attack\u001b[39m(model, target_class, patch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Leave a small set of images out to check generalization\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39m# In most of our experiments, the performance on the hold-out data points\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m# was as good as on the training set. Overfitting was little possible due\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# to the small size of the patches.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     train_set, val_set \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mrandom_split(dataset, [\u001b[39m4500\u001b[39;49m, \u001b[39m500\u001b[39;49m])\n\u001b[1;32m      7\u001b[0m     train_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(train_set, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m      8\u001b[0m     val_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(val_set, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
            "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataset.py:347\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m# Cannot verify that dataset is Sized\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msum\u001b[39m(lengths) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(dataset):    \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    349\u001b[0m indices \u001b[39m=\u001b[39m randperm(\u001b[39msum\u001b[39m(lengths), generator\u001b[39m=\u001b[39mgenerator)\u001b[39m.\u001b[39mtolist()  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[39mreturn\u001b[39;00m [Subset(dataset, indices[offset \u001b[39m-\u001b[39m length : offset]) \u001b[39mfor\u001b[39;00m offset, length \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_accumulate(lengths), lengths)]\n",
            "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
          ]
        }
      ],
      "source": [
        "#class_names = ['toaster', 'goldfish', 'school bus', 'lipstick', 'pineapple']\n",
        "class_names = label_names\n",
        "patch_sizes = [3, 5, 7, 16]\n",
        "\n",
        "patch_dict = get_patches(class_names, patch_sizes)\n",
        "save_results(patch_dict) # Uncomment if you add new class names and want to save the new results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkAFyQpWocbz"
      },
      "source": [
        "Before looking at the quantitative results, we can actually visualize the patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay7oDvdTocbz",
        "outputId": "72bcb26d-468e-4367-ab34-b03805c344e5"
      },
      "outputs": [],
      "source": [
        "def show_patches():\n",
        "    fig, ax = plt.subplots(len(patch_sizes), len(class_names), figsize=(len(class_names)*2.2, len(patch_sizes)*2.2))\n",
        "    for c_idx, cname in enumerate(class_names):\n",
        "        for p_idx, psize in enumerate(patch_sizes):\n",
        "            patch = patch_dict[cname][psize][\"patch\"]\n",
        "            patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n",
        "            patch = patch.cpu().permute(1, 2, 0).numpy()\n",
        "            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n",
        "            ax[p_idx][c_idx].imshow(patch)\n",
        "            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n",
        "            ax[p_idx][c_idx].axis('off')\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    plt.show()\n",
        "show_patches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUj14TRcocbz"
      },
      "source": [
        "We can see a clear difference between patches of different classes and sizes. In the smallest size, $32\\times 32$ pixels, some of the patches clearly resemble their class. For instance, the goldfish patch clearly shows a goldfish. The eye and the color are very characteristic of the class. Overall, the patches with $32$ pixels have very strong colors that are typical for their class (yellow school bus, pink lipstick, greenish pineapple). The larger the patch becomes, the more stretched the pattern becomes. For the goldfish, we can still spot regions that might represent eyes and the characteristic orange color, but it is not clearly a single fish anymore. For the pineapple, we might interpret the top part of the image as the leaves of pineapple fruit, but it is more abstract than our small patches. Nevertheless, we can easily spot the alignment of the patch to class, even on the largest scale.\n",
        "\n",
        "Let's now look at the quantitative results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPPj1W7locbz",
        "outputId": "343a58ba-b994-4f49-be0b-c88b84e320d8"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!-- Some HTML code to increase font size in the following table -->\n",
        "<style>\n",
        "th {font-size: 120%;}\n",
        "td {font-size: 120%;}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patch_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EviI9bQSocb0"
      },
      "outputs": [],
      "source": [
        "import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_table(top_1=True):\n",
        "    i = 0 if top_1 else 1\n",
        "    table = [[name] + [f'{(100.0 * patch_dict[name][psize][\"results\"][i]):4.2f}%' for psize in patch_sizes] for name in class_names]\n",
        "    display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Class name\"] + [f\"Patch size {psize}x{psize}\" for psize in patch_sizes])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv_2G5MZocb0"
      },
      "source": [
        "First, we will create a table of top-1 accuracy, meaning that how many images have been classified with the target class as highest prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd7QON8aocb0",
        "outputId": "c280964c-4a59-4ef9-f3c1-289044c4218b"
      },
      "outputs": [],
      "source": [
        "show_table(top_1=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEOBMBiqocb0"
      },
      "source": [
        "The clear trend, that we would also have expected, is that the larger the patch, the easier it is to fool the model. For the largest patch size of $64\\times 64$, we are able to fool the model on almost all images, despite the patch covering only 8% of the image. The smallest patch actually covers 2% of the image, which is almost neglectable. Still, the fooling accuracy is quite remarkable. A large variation can be however seen across classes. While *school bus* and *pineapple* seem to be classes that were easily predicted, *toaster* and *lipstick* seem to be much harder for creating a patch. It is hard to intuitively explain why our patches underperform on those classes. Nonetheless, a fooling accuracy of >40% is still very good for such a tiny patch.\n",
        "\n",
        "Let's also take a look at the top-5 accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilg_rNFbocb0",
        "outputId": "aafb0cef-7d20-4f4a-fe10-2dc2e0c6ac10"
      },
      "outputs": [],
      "source": [
        "show_table(top_1=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s964AEdOocb0"
      },
      "source": [
        "We see a very similar pattern across classes and patch sizes. The patch size $64$ obtains >99.7% top-5 accuracy for any class, showing that we can almost fool the network on any image. A top-5 accuracy of >70% for the hard classes and small patches is still impressive and shows how vulnerable deep CNNs are to such attacks.\n",
        "\n",
        "Finally, let's create some example visualizations of the patch attack in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgJ_roLhocb0"
      },
      "outputs": [],
      "source": [
        "def perform_patch_attack(patch):\n",
        "    patch_batch = exmp_batch.clone()\n",
        "    patch_batch = place_patch(patch_batch, patch)\n",
        "    with torch.no_grad():\n",
        "        patch_preds = pretrained_model(patch_batch.to(device))\n",
        "    for i in range(1,17,5):\n",
        "        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkTbExXWocb0",
        "outputId": "61355d11-6b7e-4c1a-86bc-e752f6e2691d"
      },
      "outputs": [],
      "source": [
        "perform_patch_attack(patch_dict['goldfish'][32]['patch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RXRk51Cocb0"
      },
      "source": [
        "The tiny goldfish patch can change all of the predictions to \"goldfish\" as top class. Note that the patch attacks work especially well if the input image is semantically similar to the target class (e.g. a fish and the target class \"goldfish\" works better than an airplane image with that patch). Nevertheless, we can also let the network predict semantically dis-similar classes by using a larger patch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOxm6dQWocb0",
        "outputId": "37eb68a6-4015-4276-d388-5ace348437ac"
      },
      "outputs": [],
      "source": [
        "perform_patch_attack(patch_dict['school bus'][64]['patch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxL0wvyAocb0"
      },
      "source": [
        "Although none of the images have anything to do with an American school bus, the high confidence of often 100% shows how powerful such attacks can be. With a few lines of code and access to the model, we were able to generate patches that we add to any image to make the model predict any class we want. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f3Gurpocb0"
      },
      "source": [
        "### Transferability of white-box attacks\n",
        "\n",
        "FGSM and the adversarial patch attack were both focused on one specific image. However, can we transfer those attacks to other models? The adversarial patch attack as proposed in [Brown et al.](https://arxiv.org/pdf/1712.09665.pdf), was originally trained on multiple models, and hence was also able to work on many different network architecture. But how different are the patches for different models anyway? For instance, let's evaluate some of our patches trained above on a different network, e.g. DenseNet121."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygskb20wocb0"
      },
      "outputs": [],
      "source": [
        "transfer_model = torchvision.models.densenet121(weights='IMAGENET1K_V1')\n",
        "transfer_model = transfer_model.to(device)\n",
        "\n",
        "# No gradients needed for the network\n",
        "transfer_model.eval()\n",
        "for p in transfer_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9LGmSJeocb0"
      },
      "source": [
        "Feel free to change the class name and/or patch size below to test out different patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J36aCBtBocb0",
        "outputId": "3ea2c7a7-de1d-4908-f49c-d270cc37e507"
      },
      "outputs": [],
      "source": [
        "class_name = 'pineapple'\n",
        "patch_size = 64\n",
        "print(f\"Testing patch \\\"{class_name}\\\" of size {patch_size}x{patch_size}\")\n",
        "\n",
        "results = eval_patch(transfer_model, \n",
        "                     patch_dict[class_name][patch_size][\"patch\"], \n",
        "                     data_loader, \n",
        "                     target_class=label_names.index(class_name))\n",
        "\n",
        "print(f\"Top-1 fool accuracy: {(results[0] * 100.0):4.2f}%\")\n",
        "print(f\"Top-5 fool accuracy: {(results[1] * 100.0):4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Y90rfDocb1"
      },
      "source": [
        "Although the fool accuracy is significantly lower than on the original ResNet34, it still has a considerable impact on DenseNet although the networks have completely different architectures and weights. If you would compare more patches and models, some would work better than others. However, one aspect which allows patch attacks to generalize well is if all the networks have been trained on the same data. In this case, all networks have been trained on ImageNet. Dataset biases make the networks recognize specific patterns in the underlying image data that humans would not have seen, and/or only work for the given dataset. This is why the knowledge of what data has been used to train a specific model is already worth a lot in the context of adversarial attacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFDV3aYkocb1"
      },
      "source": [
        "## Protecting against adversarial attacks\n",
        "\n",
        "There are many more attack strategies than just FGSM and adversarial patches that we haven't discussed and implemented ourselves here. However, what about the other perspective? What can we do to *protect* a network against adversarial attacks? The sad truth to this is: not much.\n",
        "\n",
        "White-box attacks require access to the model and its gradient calculation. The easiest way of preventing this is by ensuring safe, private storage of the model and its weights. However, some attacks, called black-box attacks, also work without access to the model's parameters, or white-box attacks can also generalize as we have seen above on our short test on transferability. \n",
        "\n",
        "So, how could we eventually protect a model? An intuitive approach would to train/finetune a model on such adversarial images, leading to an adversarial training similar to a GAN. During training, we would pretend to be the attacker, and use for example FGSM as an augmentation strategy. However, this usually just ends up in an oscillation of the defending network between weak spots. Another common trick to increase robustness against adversarial attacks is defensive distillation ([Papernot et al.](https://arxiv.org/pdf/1511.04508.pdf)). Instead of training the model on the dataset labels, we train a secondary model on the softmax predictions of the first one. This way, the loss surface is \"smoothed\" in the directions an attacker might try to exploit, and it becomes more difficult for the attacker to find adversarial examples. Nevertheless, there hasn't been found the one, true strategy that works against all possible adversarial attacks.\n",
        "\n",
        "Why are CNNs, or neural networks in general, so vulnerable to adversarial attacks? While there are many possible explanations, the most intuitive is that neural networks don't know what they don't know. Even a large dataset represents just a few sparse points in the extremely large space of possible images. A lot of the input space has not been seen by the network during training, and hence, we cannot guarantee that the prediction for those images is any useful. The network instead learns a very good classification on a smaller region, often referred to as manifold, while ignoring the points outside of it. NNs with uncertainty prediction could potentially help to discover what the network does not know. \n",
        "Another possible explanation lies in the activation function. As we know, most CNNs use ReLU-based activation functions. While those have enabled great success in training deep neural networks due to their stable gradient for positive values, they also constitute a possible flaw. The output range of a ReLU neuron can be arbitrarily high. Thus, if we design a patch or the noise in the image to cause a very high value for a single neuron, it can overpower many other features in the network. Thus, although ReLU stabilizes training, it also offers a potential point of attack for adversaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brm1JJ30ocb1"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have looked at different forms of adversarial attacks. Deep CNNs can be fooled by only slight modifications to the input. Whether it is a carefully designed noise pattern, unnoticeable for a human, or a small patch, we are able to manipulate the networks' predictions significantly. The fact that even white-box attacks can be transferable across networks, and that there exist no suitable protections against all possible adversarial attacks, make this concept a massive problem for real-world applications. While adversarial attacks can also be used for improving/training a robust model or a GAN, it is not close to being solved yet. This is also because neural networks are currently complex, unknown non-linear functions in high-dimensional looking for correlations instead of causation. In the next years, we might hopefully see an improvement in the stability of such models by using causal approaches and/or introducing uncertainty. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuRRnnqjocb1"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" ICLR 2015.\n",
        "\n",
        "[2] Hendrik Metzen, Jan, et al. \"Universal adversarial perturbations against semantic image segmentation.\" Proceedings of the IEEE International Conference on Computer Vision. 2017.\n",
        "\n",
        "[3] Anant Jain. \"Breaking neural networks with adversarial attacks.\" [Blog post](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa) 2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2iZA5Urocb1"
      },
      "source": [
        "---\n",
        "\n",
        "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider -ing our repository.    \n",
        "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e8ba96d35f524bf788e0a066ae8555804e9c3c2c45d7eaf088b255e4838dd8cc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
