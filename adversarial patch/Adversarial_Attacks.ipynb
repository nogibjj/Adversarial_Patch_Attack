{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd5uck2_ocbq"
   },
   "source": [
    "# Adversarial Patch Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btI7vLGvocbt",
    "outputId": "9d950abe-3514-4af0-b28c-195c6ad8984c"
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError:  # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = (\n",
    "    torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    ")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "# from tools\n",
    "# Move up one directory to import from the tools folder\n",
    "%cd ..\n",
    "from tools.resnet20 import ResNetCIFAR\n",
    "from tools.train_util import *\n",
    "\n",
    "# Move back to the adversarial patch folder\n",
    "!pwd\n",
    "%cd /workspaces/Adversarial_Patch_Attack/adversarial patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4vRgvPW8ocbv"
   },
   "source": [
    "## ResNet20 on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH8cCwOWocbv"
   },
   "outputs": [],
   "source": [
    "# Load ResNet20 architecture pretrained on CIFAR-10\n",
    "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
    "pretrained_model = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "pretrained_model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/workspaces/Adversarial_Patch_Attack/tools/pretrained_model_resnet20.pt\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# No gradients needed eval\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GCH-6Nt2ocbw"
   },
   "source": [
    "Perform attacks on CIFAR-10 images. Load images and create data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW83V1Vlocbw"
   },
   "outputs": [],
   "source": [
    "# Mean and Std from CIFAR10\n",
    "NORM_MEAN = np.array([0.4914, 0.4822, 0.4465])\n",
    "NORM_STD = np.array([0.2023, 0.1994, 0.2010])\n",
    "# No resizing and center crop necessary as images are already preprocessed.\n",
    "plain_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "    ]\n",
    ")\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "    ]\n",
    ")\n",
    "# Load CIFAR10 dataset\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATASET_PATH, train=True, download=True, transform=plain_transforms\n",
    ")\n",
    "data_loader = data.DataLoader(\n",
    "    dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=8\n",
    ")\n",
    "# Load CIFAR10 test set\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATASET_PATH, train=False, download=True, transform=test_transforms\n",
    ")\n",
    "test_data_loader = data.DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=8\n",
    ")\n",
    "\n",
    "# Get label names of CIFAR10 class\n",
    "label_names = dataset.classes\n",
    "print(label_names)\n",
    "\n",
    "# Get label index of CIFAR10 class\n",
    "def get_label_index(lab_str):\n",
    "    assert (\n",
    "        lab_str in label_names\n",
    "    ), f'Label \"{lab_str}\" not found. Check the spelling of the class.'\n",
    "    return label_names.index(lab_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try get_label_index\n",
    "get_label_index(\"cat\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N13cZd-Nocbw"
   },
   "source": [
    "Verify model perofrmance using the error (1 - accuracy) instead of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kySDWdLJocbw"
   },
   "outputs": [],
   "source": [
    "def eval_model(dataset_loader, img_func=None):\n",
    "    tp, counter = 0.0, 0.0\n",
    "    for imgs, labels in tqdm(dataset_loader, desc=\"Validating...\"):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if img_func is not None:\n",
    "            imgs = img_func(imgs, labels)\n",
    "        with torch.no_grad():\n",
    "            preds = pretrained_model(imgs)\n",
    "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
    "        counter += preds.shape[0]\n",
    "    acc = tp.float().item() / counter\n",
    "    print(f\"Error: {(100.0 * (1 - acc)):4.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "570f009b72a84eee807c65eecd83c735"
     ]
    },
    "id": "vxCZnrRbocbx",
    "outputId": "f99db999-f4fb-4e29-f8d0-1a76b4e7550e"
   },
   "outputs": [],
   "source": [
    "_ = eval_model(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_model(test_data_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7p_hdRkrocbx"
   },
   "source": [
    "Function show_prediction plots an image along with a bar diagram of its top 2 predictions. We also prepare it to show adversarial examples for later applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKJf8Oxiocbx"
   },
   "outputs": [],
   "source": [
    "def show_prediction(img, label, pred, K=2, adv_img=None, noise=None):\n",
    "\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        # Tensor image to numpy\n",
    "        img = img.cpu().permute(1, 2, 0).numpy()\n",
    "        img = (img * NORM_STD[None, None]) + NORM_MEAN[None, None]\n",
    "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
    "        label = label.item()\n",
    "\n",
    "    # Plot on the left the image with the true label as title.\n",
    "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
    "    if noise is None or adv_img is None:\n",
    "        fig, ax = plt.subplots(\n",
    "            1, 2, figsize=(10, 2), gridspec_kw={\"width_ratios\": [1, 1]}\n",
    "        )\n",
    "    else:\n",
    "        fig, ax = plt.subplots(\n",
    "            1, 5, figsize=(12, 2), gridspec_kw={\"width_ratios\": [1, 1, 1, 1, 2]}\n",
    "        )\n",
    "\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(label_names[label])\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    if adv_img is not None and noise is not None:\n",
    "        # Visualize adversarial images\n",
    "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
    "        adv_img = (adv_img * NORM_STD[None, None]) + NORM_MEAN[None, None]\n",
    "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
    "        ax[1].imshow(adv_img)\n",
    "        ax[1].set_title(\"Adversarial\")\n",
    "        ax[1].axis(\"off\")\n",
    "        # Visualize noise\n",
    "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
    "        noise = noise * 0.5 + 0.5  # Scale between 0 to 1\n",
    "        ax[2].imshow(noise)\n",
    "        ax[2].set_title(\"Noise\")\n",
    "        ax[2].axis(\"off\")\n",
    "        # buffer\n",
    "        ax[3].axis(\"off\")\n",
    "\n",
    "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
    "        pred = torch.softmax(pred, dim=-1)\n",
    "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
    "    print(topk_idx)\n",
    "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
    "    ax[-1].barh(\n",
    "        np.arange(K),\n",
    "        topk_vals * 100.0,\n",
    "        align=\"center\",\n",
    "        color=[\"C0\" if topk_idx[i] != label else \"C2\" for i in range(K)],\n",
    "    )\n",
    "    ax[-1].set_yticks(np.arange(K))\n",
    "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
    "    ax[-1].invert_yaxis()\n",
    "    ax[-1].set_xlabel(\"Confidence\")\n",
    "    ax[-1].set_title(\"Predictions\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I91-Ufh8ocbx"
   },
   "source": [
    "Let's visualize a few images below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6L9ha08ocbx",
    "outputId": "95f1df98-2f39-4ab6-d217-0b7701d823a1"
   },
   "outputs": [],
   "source": [
    "exmp_batch, label_batch = next(iter(test_data_loader))\n",
    "with torch.no_grad():\n",
    "    preds = pretrained_model(exmp_batch.to(device))\n",
    "\n",
    "for i in range(1, 17, 5):\n",
    "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NfgvMq4Focby"
   },
   "source": [
    "### Adversarial Patches\n",
    "\n",
    "Create an image patch to cover an area of the original image for a targetted patch attack. This image patch will cause the model to predict a specific classs using [Tom Brown et al.](https://arxiv.org/pdf/1712.09665.pdf) to learn this adversarial image patch and cause the network prediction to change. \n",
    "\n",
    "To train patch: \n",
    "1. Calculate input gradient by replacing part of input image with patch and calculate just the patch gradient\n",
    "2. Train patch with SGD so that pretrained model predicts a specific class \n",
    "\n",
    "\n",
    "Unlike [Brown et al.](https://arxiv.org/pdf/1712.09665.pdf), we did not randomly rotate or scale the patch during training before palcing it at a random place in the input image. We wonly randomly placed a patch on the image. \n",
    "\n",
    "Patch is an `nn.Parameter` with values in range $-\\infty$ and $\\infty$, but images are limited to range of CIFAR-10, so patch_forward function limits the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_MEANS, TENSOR_STD = (\n",
    "    torch.FloatTensor(NORM_MEAN)[:, None, None],\n",
    "    torch.FloatTensor(NORM_STD)[:, None, None],\n",
    ")\n",
    "\n",
    "\n",
    "def patch_forward(patch):\n",
    "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
    "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU6tbkCmocby"
   },
   "outputs": [],
   "source": [
    "# Randomly place a patch on the image\n",
    "def place_patch(img, patch):\n",
    "    for i in range(img.shape[0]):\n",
    "        h_offset = np.random.randint(0, img.shape[2] - patch.shape[1] - 1)\n",
    "        w_offset = np.random.randint(0, img.shape[3] - patch.shape[2] - 1)\n",
    "        img[\n",
    "            i,\n",
    "            :,\n",
    "            h_offset : h_offset + patch.shape[1],\n",
    "            w_offset : w_offset + patch.shape[2],\n",
    "        ] = patch_forward(patch)\n",
    "    return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UP2XhwPKocbz"
   },
   "source": [
    "Create eval_patch to evaluate success of a patch by how many time it can fool the network into predicting target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpjUc1rfocbz"
   },
   "outputs": [],
   "source": [
    "def eval_patch(model, patch, val_loader, target_class):\n",
    "    model.eval()\n",
    "    tp, counter = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
    "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
    "            for _ in range(4):\n",
    "                patch_img = place_patch(img, patch)\n",
    "                patch_img = patch_img.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                pred = model(patch_img)\n",
    "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
    "                # as we would not \"fool\" the model into predicting those\n",
    "                tp += torch.logical_and(\n",
    "                    pred.argmax(dim=-1) == target_class, img_labels != target_class\n",
    "                ).sum()\n",
    "                counter += (img_labels != target_class).sum()\n",
    "    acc = tp / counter\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x-Ui7du6ocbz"
   },
   "source": [
    "Create targeted patch_attack function which trains the patch_attack provided a model, class for patch to predict, and patch size (number of pixels) using SGD optimizer with momentum to minimize the classification loss of the model given the patch in the image. Patch will look more like class over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset size\n",
    "dataset_size = len(data_loader.dataset)\n",
    "print(f'Dataset size: {dataset_size}')\n",
    "\n",
    "# get test dataset size\n",
    "test_dataset_size = len(test_data_loader.dataset)\n",
    "print(f'Test dataset size: {test_dataset_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLdojBB2ocbz"
   },
   "outputs": [],
   "source": [
    "def patch_attack(model, target_class, patch_size=16, num_epochs=5):\n",
    "    # Leave a small set of images out to check generalization\n",
    "    # In most of our experiments, the performance on the hold-out data points\n",
    "    # was as good as on the training set. Overfitting was little possible due\n",
    "    # to the small size of the patches.\n",
    "    # split train_set and val_set by 90% and 10% of dataset_size\n",
    "    dataset_size = len(data_loader.dataset)\n",
    "    train_set, val_set = torch.utils.data.random_split(\n",
    "       dataset, [int(dataset_size * 0.9), int(dataset_size * 0.1)]\n",
    "    )\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
    "    train_loader = data.DataLoader(\n",
    "        train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8\n",
    "    )\n",
    "    val_loader = data.DataLoader(\n",
    "        val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    # Create parameter and optimizer\n",
    "    if not isinstance(patch_size, tuple):\n",
    "        patch_size = (patch_size, patch_size)\n",
    "    patch = nn.Parameter(\n",
    "        torch.zeros(3, patch_size[0], patch_size[1]), requires_grad=True\n",
    "    )\n",
    "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        t = tqdm(train_loader, leave=False)\n",
    "        for img, _ in t:\n",
    "            img = place_patch(img, patch)\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            labels = torch.zeros(\n",
    "                img.shape[0], device=pred.device, dtype=torch.long\n",
    "            ).fill_(target_class)\n",
    "            loss = loss_module(pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
    "\n",
    "    # Final validation\n",
    "    acc = eval_patch(model, patch, val_loader, target_class)\n",
    "\n",
    "    return patch.data, {\"acc\": acc.item()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y63bNeziocbz"
   },
   "source": [
    "Save pre-trained patches and results in JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXSIzNWsocbz"
   },
   "outputs": [],
   "source": [
    "# Load evaluation results of the pretrained patches\n",
    "json_results_file = os.path.join(CHECKPOINT_PATH, \"patch_results.json\")\n",
    "json_results = {}\n",
    "if os.path.isfile(json_results_file):\n",
    "    with open(json_results_file, \"r\") as f:\n",
    "        json_results = json.load(f)\n",
    "\n",
    "# If you train new patches, you can save the results via calling this function\n",
    "def save_results(patch_dict):\n",
    "    result_dict = {\n",
    "        cname: {\n",
    "            psize: [\n",
    "                t.item() if isinstance(t, torch.Tensor) else t\n",
    "                for t in patch_dict[cname][psize][\"results\"]\n",
    "            ]\n",
    "            for psize in patch_dict[cname]\n",
    "        }\n",
    "        for cname in patch_dict\n",
    "    }\n",
    "    with open(os.path.join(CHECKPOINT_PATH, \"patch_results.json\"), \"w\") as f:\n",
    "        json.dump(result_dict, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HT2Zj9uUocbz"
   },
   "source": [
    "Create get_patches function to train and evaluate patches for all 10 classes in CIFAR10 on patch sizes $3\\times3$ pixels, $5\\times5$ pixels, $7\\times7$ pixels, and $16\\times16$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hRC25Vvocbz"
   },
   "outputs": [],
   "source": [
    "def get_patches(class_names, patch_sizes):\n",
    "    result_dict = dict()\n",
    "\n",
    "    # Loop over all classes and patch sizes\n",
    "    for name in class_names:\n",
    "        result_dict[name] = dict()\n",
    "        for patch_size in patch_sizes:\n",
    "            c = label_names.index(name)\n",
    "            file_name = os.path.join(CHECKPOINT_PATH, f\"{name}_{patch_size}_patch.pt\")\n",
    "            # Load patch if pretrained file exists, otherwise start training\n",
    "            if not os.path.isfile(file_name):\n",
    "                patch, val_results = patch_attack(\n",
    "                    pretrained_model,\n",
    "                    target_class=c,\n",
    "                    patch_size=patch_size,\n",
    "                    num_epochs=5,\n",
    "                )\n",
    "                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n",
    "                torch.save(patch, file_name)\n",
    "            else:\n",
    "                patch = torch.load(file_name)\n",
    "            # Load evaluation results if exist, otherwise manually evaluate the patch\n",
    "            if name in json_results:\n",
    "                results = json_results[name][str(patch_size)]\n",
    "            else:\n",
    "                results = eval_patch(\n",
    "                    pretrained_model, patch, test_data_loader, target_class=c\n",
    "                )\n",
    "\n",
    "            # Store results and the patches in a dict for better access\n",
    "            result_dict[name][patch_size] = {\"results\": results, \"patch\": patch}\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjh8NVD6ocbz"
   },
   "outputs": [],
   "source": [
    "class_names = label_names\n",
    "patch_sizes = [3, 5, 7, 16]\n",
    "\n",
    "patch_dict = get_patches(class_names, patch_sizes)\n",
    "# save_results(\n",
    "#    patch_dict\n",
    "# )  # Uncomment if you add new class names and want to save the new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(\n",
    "    patch_dict\n",
    ")  # Uncomment if you add new class names and want to save the new results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nkAFyQpWocbz"
   },
   "source": [
    "Create show_patches function to visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ay7oDvdTocbz",
    "outputId": "72bcb26d-468e-4367-ab34-b03805c344e5"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "def show_patches():\n",
    "    fig, ax = plt.subplots(\n",
    "        len(patch_sizes),\n",
    "        len(class_names),\n",
    "        figsize=(len(class_names) * 2.2, len(patch_sizes) * 2.2),\n",
    "    )\n",
    "    for c_idx, cname in enumerate(class_names):\n",
    "        for p_idx, psize in enumerate(patch_sizes):\n",
    "            patch = patch_dict[cname][psize][\"patch\"]\n",
    "            patch = (torch.tanh(patch) + 1) / 2  # Parameter to pixel values\n",
    "            patch = patch.cpu().permute(1, 2, 0).numpy()\n",
    "            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n",
    "            ax[p_idx][c_idx].imshow(patch)\n",
    "            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n",
    "            ax[p_idx][c_idx].axis(\"off\")\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_patches()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nUj14TRcocbz"
   },
   "source": [
    "Results above show patches with different classes and sizes. $16\\times 16$ pixels is the smallest size, and it more closesly resembles their classes (e.g. horse) vs. larger patch sizes like $3\\times 3$ in terms of color and image. For larger sizes (3x3 to 7x7), you cannot tell that they represent the class. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPPj1W7locbz",
    "outputId": "343a58ba-b994-4f49-be0b-c88b84e320d8"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Some HTML code to increase font size in the following table -->\n",
    "<style>\n",
    "th {font-size: 120%;}\n",
    "td {font-size: 120%;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EviI9bQSocb0"
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_table(top_1=True):\n",
    "    i = 0 if top_1 else 1\n",
    "    table = [\n",
    "        [name]\n",
    "        + [\n",
    "            f'{(100.0 * patch_dict[name][psize][\"results\"][i]):4.2f}%'\n",
    "            for psize in patch_sizes\n",
    "        ]\n",
    "        for name in class_names\n",
    "    ]\n",
    "    display(\n",
    "        HTML(\n",
    "            tabulate.tabulate(\n",
    "                table,\n",
    "                tablefmt=\"html\",\n",
    "                headers=[\"Class name\"]\n",
    "                + [f\"Patch size {psize}x{psize}\" for psize in patch_sizes],\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv_2G5MZocb0"
   },
   "source": [
    "First, we will create a table of top-1 accuracy, meaning that how many images have been classified with the target class as highest prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zd7QON8aocb0",
    "outputId": "c280964c-4a59-4ef9-f3c1-289044c4218b"
   },
   "outputs": [],
   "source": [
    "show_table(top_1=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rEOBMBiqocb0"
   },
   "source": [
    "As patch size increases, the targeted patch attack performs better in terms of fooling the model, which accuracy of 99% to 100% for the largest patch size of $16\\times 16$. The smallest patch $3\\times 3$ has a very poor fooling accuracy. Horse, ship, and deer classes were more difficult to create a patch for patches $5\\times 5$ and $7\\times 7$. Some classes were more easily predicted than other classes. With just patch size $7\\times 7$, the targetted attack patch performs >40% for fooling the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgJ_roLhocb0"
   },
   "outputs": [],
   "source": [
    "exmp_batch, label_batch = next(iter(test_data_loader))\n",
    "\n",
    "\n",
    "def perform_patch_attack(patch):\n",
    "    patch_batch = exmp_batch.clone()\n",
    "    patch_batch = place_patch(patch_batch, patch)\n",
    "    with torch.no_grad():\n",
    "        patch_preds = pretrained_model(patch_batch.to(device))\n",
    "    for i in range(1, 17, 5):\n",
    "        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkTbExXWocb0",
    "outputId": "61355d11-6b7e-4c1a-86bc-e752f6e2691d"
   },
   "outputs": [],
   "source": [
    "perform_patch_attack(patch_dict[\"bird\"][3][\"patch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RXRk51Cocb0"
   },
   "source": [
    "The tiny goldfish patch can change all of the predictions to \"goldfish\" as top class. Note that the patch attacks work especially well if the input image is semantically similar to the target class (e.g. a fish and the target class \"goldfish\" works better than an airplane image with that patch). Nevertheless, we can also let the network predict semantically dis-similar classes by using a larger patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOxm6dQWocb0",
    "outputId": "37eb68a6-4015-4276-d388-5ace348437ac"
   },
   "outputs": [],
   "source": [
    "perform_patch_attack(patch_dict[\"ship\"][16][\"patch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxL0wvyAocb0"
   },
   "source": [
    "Although none of the images have anything to do with an American school bus, the high confidence of often 100% shows how powerful such attacks can be. With a few lines of code and access to the model, we were able to generate patches that we add to any image to make the model predict any class we want. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f6f3Gurpocb0"
   },
   "source": [
    "### Transferability of white-box attacks\n",
    "\n",
    "Evaluate some of our patches trained above on VGG and Densenet networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densenet121 with pretrained CIFAR10 weights\n",
    "# import vgg from PyTorch_CIFAR10 folder in another directory and load the pretrained model\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../PyTorch_CIFAR10\")\n",
    "from cifar10_models.vgg import vgg11_bn, vgg13_bn, vgg16_bn, vgg19_bn\n",
    "from cifar10_models.densenet import densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ygskb20wocb0"
   },
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "transfer_model = vgg11_bn(pretrained=True)\n",
    "transfer_model = transfer_model.to(device)\n",
    "\n",
    "# No gradients needed for the network\n",
    "transfer_model.eval()\n",
    "for p in transfer_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J36aCBtBocb0",
    "outputId": "3ea2c7a7-de1d-4908-f49c-d270cc37e507"
   },
   "outputs": [],
   "source": [
    "class_name = \"cat\"\n",
    "patch_size = 16\n",
    "print(f'Testing patch \"{class_name}\" of size {patch_size}x{patch_size}')\n",
    "\n",
    "results = eval_patch(\n",
    "    transfer_model,\n",
    "    patch_dict[class_name][patch_size][\"patch\"],\n",
    "    test_data_loader,\n",
    "    target_class=label_names.index(class_name),\n",
    ")\n",
    "\n",
    "print(f\"Top-1 fool accuracy: {(results * 100.0):4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try densenet model with pretrained CIFAR10 weights\n",
    "# Pretrained model\n",
    "densenet_model = densenet121(pretrained=True)\n",
    "densenet_model = densenet_model.to(device)\n",
    "\n",
    "# No gradients needed for the network\n",
    "densenet_model.eval()\n",
    "for p in densenet_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"cat\"\n",
    "patch_size = 16\n",
    "print(f'Testing patch \"{class_name}\" of size {patch_size}x{patch_size}')\n",
    "\n",
    "results = eval_patch(\n",
    "    densenet_model,\n",
    "    patch_dict[class_name][patch_size][\"patch\"],\n",
    "    test_data_loader,\n",
    "    target_class=label_names.index(class_name),\n",
    ")\n",
    "\n",
    "print(f\"Top-1 fool accuracy: {(results * 100.0):4.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Y90rfDocb1"
   },
   "source": [
    "Although the fool accuracy is significantly lower than on the original ResNet20, it still has a considerable impact on DenseNet although the networks have completely different architectures and weights. Patch generalizes well because the models have all been trained on the same CIFAR10 data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuRRnnqjocb1"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" ICLR 2015.\n",
    "\n",
    "[2] Hendrik Metzen, Jan, et al. \"Universal adversarial perturbations against semantic image segmentation.\" Proceedings of the IEEE International Conference on Computer Vision. 2017.\n",
    "\n",
    "[3] Anant Jain. \"Breaking neural networks with adversarial attacks.\" [Blog post](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa) 2019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8ba96d35f524bf788e0a066ae8555804e9c3c2c45d7eaf088b255e4838dd8cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
